{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2f4951b8-2cd5-4d4f-a1f1-77e9a9bc614d",
      "metadata": {
        "id": "2f4951b8-2cd5-4d4f-a1f1-77e9a9bc614d"
      },
      "source": [
        "## 爬取空气质量数据"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypinyin -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4pwvNtv7vSj",
        "outputId": "efbc678d-2d9a-4c79-d22b-c53dcd9a8aa6"
      },
      "id": "L4pwvNtv7vSj",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/834.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/834.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.5/834.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b08deeb5-3d7f-4838-8a87-cb98b557074d",
      "metadata": {
        "tags": [],
        "id": "b08deeb5-3d7f-4838-8a87-cb98b557074d"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "import pypinyin\n",
        "import threading\n",
        "import pandas as pd\n",
        "from queue import Queue\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class PM_DATA:\n",
        "    '''\n",
        "    获取天气后报网站城市天气PM数据，多线程版本,存入excel\n",
        "    '''\n",
        "    def __init__(self, name, save_filename):\n",
        "\n",
        "        # 数据存入EXCEL的字典格式\n",
        "        self.index_list = ['日期', '质量等级', 'AQI指数', '当天AQI排名', 'PM2.5', 'PM10', 'So2', 'No2', 'Co', 'O3']\n",
        "        self.total_list = {}\n",
        "        self.total_list['日期'] = []\n",
        "        self.total_list['质量等级'] = []\n",
        "        self.total_list['AQI指数'] = []\n",
        "        self.total_list['当天AQI排名'] = []\n",
        "        self.total_list['PM2.5'] = []\n",
        "        self.total_list['PM10'] = []\n",
        "        self.total_list['So2'] = []\n",
        "        self.total_list['No2'] = []\n",
        "        self.total_list['Co'] = []\n",
        "        self.total_list['O3'] = []\n",
        "\n",
        "        self.name = change_c(name)\n",
        "        self.main_url = 'http://www.tianqihoubao.com/aqi/'\n",
        "        self.headers = {\n",
        "            # TODO 改为自己电脑的User-Agent\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.100 Safari/537.36',\n",
        "            'Host': 'www.tianqihoubao.com',\n",
        "            'Referer': 'http://www.tianqihoubao.com/aqi/'\n",
        "        }\n",
        "\n",
        "        self.save_filename = save_filename\n",
        "\n",
        "    def index_request(self):\n",
        "        '''\n",
        "        某城市页请求\n",
        "        :return: HTML数据\n",
        "        '''\n",
        "        r = requests.get(self.main_url + self.name + '.html', headers=self.headers)\n",
        "        # 判断请求是否成功\n",
        "        if r.status_code == requests.codes.ok:\n",
        "            print(r.status_code, '请求成功')\n",
        "            return r.text\n",
        "        else:\n",
        "            print(r.status_code, '请求失败')\n",
        "            return None\n",
        "\n",
        "    def parse_index_html(self, queue):\n",
        "        '''\n",
        "        解析该城市提供的所有PM日期URL链接\n",
        "        :return: 日期列表\n",
        "        '''\n",
        "        index_html = self.index_request()\n",
        "        if index_html is None:\n",
        "            return False\n",
        "        soup = BeautifulSoup(index_html, 'html.parser')  # 格式化HTML\n",
        "        try:\n",
        "            # 根据HTML结构，解析HTML  ---> 观察下网页的HTML的结构就了解了\n",
        "            content = soup.select_one('div.box.p').find_all_next('ul')[0]\n",
        "            son_url = content.find_all('a')\n",
        "            for attr in son_url:\n",
        "                # 从HTML的href属性中解析出URL，并对日期进行过滤\n",
        "                href_str = attr['href']\n",
        "                fir_date = href_str.split('-')\n",
        "                fin_date = fir_date[1].split('.')[0]\n",
        "                if fin_date <= '202412' and fin_date >= '202401':\n",
        "                    url = self.main_url + self.name + '-' + fin_date + '.html'\n",
        "                    queue.put(url)  # 将子url放入队列\n",
        "        except:\n",
        "            print('主HTML解析失败（可能页面被更新过）')\n",
        "\n",
        "    def son_request(self, detail_url_list):\n",
        "        '''\n",
        "        某城市月份PM数据请求\n",
        "        :return: 获取的总数据 ---> 字典\n",
        "        '''\n",
        "        while True:\n",
        "            url = detail_url_list.get()  # Queue队列的get方法用于从队列中提取元素\n",
        "            print('爬取子url：', url)\n",
        "            r = requests.get(url, headers=self.headers)\n",
        "            self.parse_son_html(r.text)\n",
        "            # 队列为空时退出循环\n",
        "            if detail_url_list.qsize() == 0:\n",
        "                break\n",
        "\n",
        "    def parse_son_html(self, son_html):\n",
        "        '''\n",
        "        解析子页面,获取PM数据\n",
        "        :param son_html: 子页面HTML数据\n",
        "        :param date_num: 月份\n",
        "        :return: 月级PM数据 ---> 字典\n",
        "        '''\n",
        "        soup = BeautifulSoup(son_html, 'html.parser')\n",
        "        try:\n",
        "            # 解析HTML中的table\n",
        "            content = soup.table.find_all('tr')\n",
        "            for index, value in enumerate(content):\n",
        "                if index == 0:   # 过滤table中的索引名字\n",
        "                    pass\n",
        "                else:\n",
        "                    for i, j in enumerate(value.find_all('td')):\n",
        "                        # 清除字符串包含的特殊字符\n",
        "                        a = re.compile(r'\\n|&nbsp|\\xa0|\\\\xa0|\\u3000|\\\\u3000|\\\\u0020|\\u0020|\\t|\\r')\n",
        "                        clean_str = a.sub('', j.string)\n",
        "                        self.total_list[self.index_list[i]].append(clean_str)\n",
        "            print('数据爬取成功！')\n",
        "        except:\n",
        "            print('子HTML解析失败（可能页面被更新过）')\n",
        "\n",
        "    def main(self):\n",
        "        '''\n",
        "        入口函数\n",
        "        :return:\n",
        "        '''\n",
        "        detail_url_queue = Queue(maxsize=1000)\n",
        "        thread = threading.Thread(target=self.parse_index_html, args=(detail_url_queue,))  # 该线程负责填充子url队列\n",
        "        html_thread = []\n",
        "        for i in range(4):\n",
        "            thread2 = threading.Thread(target=self.son_request, args=(detail_url_queue,))  # 该线程负责爬取子url数据\n",
        "            html_thread.append(thread2)\n",
        "        # 启动四个线程\n",
        "        thread.start()\n",
        "        for i in range(4):\n",
        "            html_thread[i].start()\n",
        "        thread.join()\n",
        "        for i in range(4):\n",
        "            html_thread[i].join()\n",
        "\n",
        "        # 写入EXCEL\n",
        "        self.data_to_excel()\n",
        "\n",
        "    def data_to_excel(self):\n",
        "        # 数据格式转置\n",
        "        my_df = pd.DataFrame.from_dict(self.total_list, orient='index').T\n",
        "        my_df.to_excel(self.save_filename, index=False)\n",
        "        print('写入成功')\n",
        "\n",
        "def change_c(word):\n",
        "    '''\n",
        "    将用户输入的中文转为拼音字母\n",
        "    :param word:\n",
        "    :return:\n",
        "    '''\n",
        "    s = ''\n",
        "    for i in pypinyin.pinyin(word, style=pypinyin.NORMAL):\n",
        "        s += ''.join(i)\n",
        "    return s\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    default_city = '徐州'\n",
        "    default_filename = 'PM25_data_xuzhou.xlsx'\n",
        "\n",
        "    str_value = input(f'请输入城市名（默认值：{default_city}）：') or default_city\n",
        "    filename = input(f'请输入保存文件名（包括文件后缀，如{default_filename}，默认值：{default_filename}）：') or default_filename\n",
        "\n",
        "    print('城市名：', change_c(str_value))\n",
        "    PM_DATA(str_value, filename).main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a768801e-7995-49c5-8b08-9ed341fbed7a",
      "metadata": {
        "id": "a768801e-7995-49c5-8b08-9ed341fbed7a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "999e4c4a-6ac8-4eb5-a714-16c524c21a29",
      "metadata": {
        "id": "999e4c4a-6ac8-4eb5-a714-16c524c21a29"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80f040c2-e58e-436d-ac0d-122bd27164a3",
      "metadata": {
        "id": "80f040c2-e58e-436d-ac0d-122bd27164a3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}