{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/xyt556/Data/blob/main/geoparquet_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7y7W2T-aFLfg"
   },
   "source": [
    "# GeoParquet 数据分析\n",
    "\n",
    "本笔记本用于使用 GeoParquet 文件格式进行基本的地理空间分析。输入数据可以从 [nz-building-outlines.parquet](https://storage.googleapis.com/open-geodata/linz-examples/nz-building-outlines.parquet) 下载。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ih2gpeEFXzh"
   },
   "source": [
    "GeoParquet 是一种基于 Parquet 文件格式的开源地理数据存储格式。它将地理空间数据扩展到 Parquet 文件，使其能够高效地存储和查询大规模的地理空间数据集。\n",
    "\n",
    "### 主要特性\n",
    "1. **基于 Parquet 格式**：GeoParquet 建立在 Apache Parquet 格式之上，Parquet 是一种列式存储格式，适合大规模数据分析。它能有效压缩和存储数据，同时支持高效的读写操作。\n",
    "\n",
    "2. **地理空间支持**：GeoParquet 在 Parquet 的基础上，增加了对几何对象（如点、线、多边形等）的支持。它通过在文件中添加适当的元数据来处理几何列，从而允许处理复杂的地理空间数据。\n",
    "\n",
    "3. **多种几何格式支持**：GeoParquet 支持常见的几何格式如 WKT (Well-Known Text) 和 WKB (Well-Known Binary)，使得它能够灵活地与各种 GIS 软件和库集成。\n",
    "\n",
    "4. **与现代数据生态系统兼容**：由于 GeoParquet 是基于 Parquet 的，所以它可以无缝集成到 Apache Arrow、Spark、Flink 等数据处理框架中。这意味着可以利用分布式计算工具对大规模地理空间数据集进行分析。\n",
    "\n",
    "5. **高效的数据压缩**：Parquet 的列式存储和压缩特性意味着在处理大规模地理数据时，GeoParquet 可以显著降低存储空间和加快数据读取速度。\n",
    "\n",
    "### 适用场景\n",
    "GeoParquet 特别适合以下场景：\n",
    "- **大规模地理空间数据分析**：例如城市规划、环境监测、灾害预防等领域的高频率大数据计算。\n",
    "- **与大数据处理框架集成**：GeoParquet 可以用于分布式数据处理平台中的地理空间数据分析，比如使用 Apache Spark 处理地理空间数据。\n",
    "- **高效存储和传输**：适合处理需要低存储占用和高效传输的大规模地理数据集。\n",
    "\n",
    "### 使用示例\n",
    "使用 GeoParquet 可以与 Python 中的 GeoPandas 库配合，用于加载和存储地理空间数据。例如：\n",
    "\n",
    "```python\n",
    "import geopandas as gpd\n",
    "df = gpd.read_file(\"your_geo_data.shp\")  # 读取Shapefile\n",
    "df.to_parquet(\"your_data.parquet\")  # 保存为GeoParquet格式\n",
    "```\n",
    "\n",
    "总的来说，GeoParquet 是一种高效、灵活的存储格式，适用于大规模的地理空间数据分析，并且可以无缝地融入现代数据分析和处理生态系统中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bWRAJLytNtAf",
    "outputId": "18e70668-7f7b-4fd3-a821-1a17b6cd62f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "# 从github上下载所需要的文件\n",
    "!wget https://github.com/xyt556/Data/raw/main/mytools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJ2VSukBqiRC",
    "outputId": "7b73efcc-9f65-44fb-c7dd-ca5d95822c31",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 URLs:\n",
      "https://github.com/xyt556/Data/raw/main/font_colab.ipynb\n",
      "文件 font_colab.ipynb 已下载并保存到 .\n",
      "https://github.com/xyt556/Data/raw/main/set_font.ipynb\n",
      "文件 set_font.ipynb 已下载并保存到 .\n"
     ]
    }
   ],
   "source": [
    "# 自定义的函数，用与下载数据\n",
    "from mytools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVRSHHCVHGA8",
    "outputId": "ec77620a-ba31-4d79-c637-8c6aa8654b1f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zQaJ8WTWxCNb3D0ZrC5bo5KvaT0cNC6u\n",
      "To: C:\\Data\\nz-building-outlines.parquet\n",
      "  9%|████████████▍                                                                                                                                   | 37.2M/430M [00:04<00:46, 8.51MB/s]('Connection broken: IncompleteRead(37224448 bytes read, 392922440 more expected)', IncompleteRead(37224448 bytes read, 392922440 more expected))\n",
      "  9%|████████████▍                                                                                                                                   | 37.2M/430M [00:04<00:43, 8.95MB/s]\n"
     ]
    }
   ],
   "source": [
    "# 从Google网盘的共享文件\n",
    "download_file_from_google_drive(\"https://drive.google.com/file/d/1zQaJ8WTWxCNb3D0ZrC5bo5KvaT0cNC6u/view?usp=sharing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "feRnRtA9FLfi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -q pyarrow\n",
    "# ! pip install geopandas==1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgBc2TL3-HvU"
   },
   "source": [
    "`pyarrow` 是一个高效的 Python 库，用于在 Python 应用程序和 Apache Arrow 之间进行交互。Apache Arrow 是一种跨语言的内存格式，可以快速高效地转移大型数据集。以下是对 `pyarrow` 的详细介绍：\n",
    "\n",
    "### 功能\n",
    "1. **数据传输和存储**：`pyarrow` 提供了跨平台、高性能的数据传输和存储解决方案，支持序列化和分布式读取等功能。\n",
    "2. **与其他库的集成**：`pyarrow` 与 NumPy、pandas 等 Python 生态系统中的其他软件具有一流的集成能力。\n",
    "3. **内存分析**：`pyarrow` 是一个用于内存分析的平台，包含一组技术，使大数据系统能够快速存储、处理和移动数据。\n",
    "\n",
    "### 安装\n",
    "可以使用 `pip` 安装：\n",
    "```bash\n",
    "pip install pyarrow\n",
    "```\n",
    "也可以使用 `conda` 安装：\n",
    "```bash\n",
    "conda install pyarrow -c conda-forge\n",
    "```\n",
    "\n",
    "### 使用示例\n",
    "1. **创建数组和表**：\n",
    "```python\n",
    "import pyarrow as pa\n",
    "\n",
    "# 创建一个 Arrow 数组\n",
    "array = pa.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# 创建一个 Arrow 表\n",
    "table = pa.table({'column1': array})\n",
    "print(table)\n",
    "```\n",
    "\n",
    "2. **读取和写入 Parquet 文件**：\n",
    "```python\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# 写入 Parquet 文件\n",
    "pq.write_table(table, 'example.parquet')\n",
    "\n",
    "# 读取 Parquet 文件\n",
    "table = pq.read_table('example.parquet')\n",
    "print(table)\n",
    "```\n",
    "\n",
    "### 适用场景\n",
    "- **大规模数据分析**：适用于需要高效处理和传输大规模数据集的场景，如数据分析、机器学习和大数据处理。\n",
    "- **与现代数据生态系统兼容**：由于 `pyarrow` 基于 Apache Arrow，因此它可以无缝集成到 Apache Arrow、Spark、Flink 等数据处理框架中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRnBGqd0FLfi"
   },
   "source": [
    "## 读写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ihnVi7L-FLfj",
    "outputId": "6e500c8c-cc9c-4295-88f1-e597276a040b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gpd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eXhNGKLzGAs8"
   },
   "outputs": [],
   "source": [
    "# fold = '/content/drive/MyDrive/Colab Notebooks/geospatial-data-analysis-python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "xVm3z4w0FLfk",
    "outputId": "e8a6cd56-71b5-48f6-d3d5-e75a3f4ada73"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "nz-building-outlines.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 加载 geoparquet 文件\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m gdf \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnz-building-outlines.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m gdf\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mD:\\Geo_Python_2024\\envi\\lib\\site-packages\\geopandas\\io\\arrow.py:604\u001b[0m, in \u001b[0;36m_read_parquet\u001b[1;34m(path, columns, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m path \u001b[38;5;241m=\u001b[39m _expand_user(path)\n\u001b[0;32m    603\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_pandas_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 604\u001b[0m table \u001b[38;5;241m=\u001b[39m parquet\u001b[38;5;241m.\u001b[39mread_table(path, columns\u001b[38;5;241m=\u001b[39mcolumns, filesystem\u001b[38;5;241m=\u001b[39mfilesystem, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    606\u001b[0m \u001b[38;5;66;03m# read metadata separately to get the raw Parquet FileMetaData metadata\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;66;03m# (pyarrow doesn't properly exposes those in schema.metadata for files\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;66;03m# created by GDAL - https://issues.apache.org/jira/browse/ARROW-16688)\u001b[39;00m\n\u001b[0;32m    609\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Geo_Python_2024\\envi\\lib\\site-packages\\pyarrow\\parquet\\core.py:2824\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   2817\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2818\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword is no longer supported with the new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2819\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets-based implementation. Specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to temporarily recover the old \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2821\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbehaviour.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2822\u001b[0m     )\n\u001b[0;32m   2823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2824\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43m_ParquetDatasetV2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2825\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2838\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2839\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   2840\u001b[0m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[0;32m   2841\u001b[0m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[0;32m   2842\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Geo_Python_2024\\envi\\lib\\site-packages\\pyarrow\\parquet\\core.py:2423\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[1;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001b[0m\n\u001b[0;32m   2419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partitioning \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2420\u001b[0m     partitioning \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mHivePartitioning\u001b[38;5;241m.\u001b[39mdiscover(\n\u001b[0;32m   2421\u001b[0m         infer_dictionary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2423\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2424\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2425\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2426\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Geo_Python_2024\\envi\\lib\\site-packages\\pyarrow\\dataset.py:752\u001b[0m, in \u001b[0;36mdataset\u001b[1;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[0;32m    741\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    742\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m    743\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    748\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mignore_prefixes\n\u001b[0;32m    749\u001b[0m )\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[1;32m--> 752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "File \u001b[1;32mD:\\Geo_Python_2024\\envi\\lib\\site-packages\\pyarrow\\dataset.py:444\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[1;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[0;32m    442\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_single_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m options \u001b[38;5;241m=\u001b[39m FileSystemFactoryOptions(\n\u001b[0;32m    447\u001b[0m     partitioning\u001b[38;5;241m=\u001b[39mpartitioning,\n\u001b[0;32m    448\u001b[0m     partition_base_dir\u001b[38;5;241m=\u001b[39mpartition_base_dir,\n\u001b[0;32m    449\u001b[0m     exclude_invalid_files\u001b[38;5;241m=\u001b[39mexclude_invalid_files,\n\u001b[0;32m    450\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mselector_ignore_prefixes\n\u001b[0;32m    451\u001b[0m )\n\u001b[0;32m    452\u001b[0m factory \u001b[38;5;241m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n",
      "File \u001b[1;32mD:\\Geo_Python_2024\\envi\\lib\\site-packages\\pyarrow\\dataset.py:420\u001b[0m, in \u001b[0;36m_ensure_single_source\u001b[1;34m(path, filesystem)\u001b[0m\n\u001b[0;32m    418\u001b[0m     paths_or_selector \u001b[38;5;241m=\u001b[39m [path]\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: nz-building-outlines.parquet"
     ]
    }
   ],
   "source": [
    "# 加载 geoparquet 文件\n",
    "gdf = gpd.read_parquet('nz-building-outlines.parquet')\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJb2QMY4FLfk",
    "outputId": "c23af01c-8c0f-4bd1-df5e-00b69c4b7256"
   },
   "outputs": [],
   "source": [
    "len(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pm5CEikmFLfl"
   },
   "outputs": [],
   "source": [
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJHQuzduFLfl"
   },
   "outputs": [],
   "source": [
    "gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XA2zaOE5FLfm"
   },
   "outputs": [],
   "source": [
    "gdf.to_file('nz-building-outlines.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVv_vGoVFLfn"
   },
   "outputs": [],
   "source": [
    "gdf.to_file('nz-building-outlines.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fnWeILRFLfo"
   },
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('nz-building-outlines.geojson')\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yEyRLatFLfp"
   },
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('nz-building-outlines.shp')\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IS5uAXp2FLfp"
   },
   "outputs": [],
   "source": [
    "gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FH4HjjyaFLfq"
   },
   "outputs": [],
   "source": [
    "gdf.to_parquet('nz-building-outlines1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnQ1eE3oFLfq"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print('-'*50)\n",
    "start_p = datetime.now()\n",
    "gdf = gpd.read_parquet('nz-building-outlines.parquet')\n",
    "end_p = datetime.now()\n",
    "print('Parquet data read time: ', end_p - start_p)\n",
    "\n",
    "start_s = datetime.now()\n",
    "gdf = gpd.read_file('shp/nz-building-outlines.shp')\n",
    "end_s = datetime.now()\n",
    "print('Shapefile data read time: ', end_s - start_s)\n",
    "\n",
    "start_g = datetime.now()\n",
    "gdf = gpd.read_file('nz-building-outlines.geojson')\n",
    "end_g = datetime.now()\n",
    "print('GeoJSON data read time: ', end_g - start_g)\n",
    "\n",
    "print('-'*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSlAAY1uFLfq"
   },
   "source": [
    "## 空间分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4frL1DC6FLfr"
   },
   "outputs": [],
   "source": [
    "# 加载 geoparquet 文件\n",
    "gdf = gpd.read_parquet('nz-building-outlines.parquet')\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KK8v7aScFLfr"
   },
   "outputs": [],
   "source": [
    "# 根据属性过滤数据\n",
    "gdf_filtered = gdf[gdf['town_city'] == 'Marton']\n",
    "\n",
    "# 10M的缓冲区\n",
    "gdf_filtered['geometry'] = gdf_filtered.buffer(10)\n",
    "\n",
    "# 读取前10000个数据\n",
    "gdf_filtered = gdf_filtered.head(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieLxGKosFLfr"
   },
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "PK-jJDPQFLfr"
   },
   "outputs": [],
   "source": [
    "# 绘图\n",
    "gdf_filtered.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BiPZyCw7FLfs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "gdf_filtered.to_file('marton-buildings-buffered.geojson', driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
